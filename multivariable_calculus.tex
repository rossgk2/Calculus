\part*{Multivariable calculus}

\subsection*{References}

\begin{itemize}
    \item \textit{The Calculus of Several Variables} by Robert C Rogers of Nagoya University in Japan: \url{https://www.math.nagoya-u.ac.jp/~richard/teaching/s2016/Ref2.pdf}
    \item \textit{Vector Calculus, Linear Algebra, and Differential Forms} by Hubbard and Hubbard
    \item \textit{Calculus on Manifolds} by Michael Spivak
\end{itemize}

\subsection*{Lin alg teaser}

\begin{itemize}
    \item Copy-paste discussion of $\R^n$ over from linear algebra (or find a way to allow this and the linear algebra file to use same dependency)
    \item Define linear function, emphasize that contains the vectors doesn't have to be $\R^n$, the set of ``vectors'' could be a set of functions, example- $\frac{d}{dx}$ and $\int$ are linear functions, but \textit{don't} define vector space
    \item Matrix-vector product and matrix-matrix product
    \item Dot product
    \item Cross product
    \begin{itemize}
        \item For $\vv \in \R^3$, derive the matrix of the linear function $\ww \mapsto \vv \times \ww$ to explain the cross product formula.
        \item Prove that cross product satisfies the ``right hand rule'' (i.e. puts up out of the plane spanned by $\vv$ and $\ww$ if the counterclockwise angle from $\vv$ to $\ww$ is $\leq \frac{\pi}{2}$) by checking it for the standard basis.
        \begin{itemize}
            \item Footnote: in the approach we've used to define the cross product, the right hand rule is imposed by deciding that we should have $\see_1 \times \see_2 = \see_3$, $\see_3 \times \see_1 = \see_2$, and $\see_2 \times \see_3 = \see_1$. When the cross product is defined in the best possible way, the right hand rule is not imposed by the mathematician- it is a natural consequence of other facts. I don't present this best possible way here because it requires knowledge of what are called \textit{determinants}. If you are interested, read [...my lin alg book...] for a presentation of the best possible way.
            \item Point the reader to my linear algebra text for the reason as to why the right hand rule arises. It all starts with noticing that the ordered basis $\{\see_1, \see_2\}$ is rotationally equivalent to $\{-\see_2, \see_1\}$.
        \end{itemize}
    \end{itemize}
\end{itemize}

\newpage

\section*{Differentiation}

\subsection*{Introduction}

One might think that it is best to start a study of multivariable calculus with the general definition of differentiability for functions $\R^n \rightarrow \R^m$. Let's think about this. The fullest understanding of this notion of differentiability involves an understanding that a function $\R^n \rightarrow \R^m$ may have directional derivatives defined in all directions but still fail to be differentiable, since general differentiability is equivalent to having directional derivatives defined in all directions \textit{and} having these directional derivatives vary linearly with the direction vector. In order to grasp this, one must understand directional derivatives of functions $\R^n \rightarrow \R^m$. To understand these generalized directional derivatives, one must understand what partial derivatives of functions $\R^n \rightarrow \R^m$ (e.g. $\frac{\boldsymbol{\pd} \ff(x_1, ..., x_i, ..., x_n)}{\boldsymbol{\pd} x_i}$). And to understand these generalized partial derivatives, one must understand the partial derivative of a scalar-valued function (e.g. $\frac{\pd f(x_1, ..., x_i, ..., x_n)}{\pd x_i}$), and the scalar derivative of a vector valued function (e.g. $\frac{d\ff(t)}{dt}$)!

So, one could argue that all of these concepts are prerequisites for general differentiability. True, one could also argue that defining generalized directional derivatives and generalized partial partial derivatives doesn't really require that much motivation, but it is at least very reasonable to have introduced all of these concepts before reaching general differentiability. That's good, because it seems quite intuitive and natural to stumble on the more rudimentary concepts first.

\subsection*{Outline}

First, study functions $\R \rightarrow \R^n$.

\begin{itemize}
    \item Define derivatives and integrals of functions $\R^n \rightarrow \R$. 
    \item Study space curves and derive the Frenet frame.
\end{itemize}

Next, study functions $\R^n \rightarrow \R$.
    
\begin{itemize}
    \item Define the directional derivative of $f:\R^n \rightarrow \R$ as $\frac{\pd f}{\pd \vv} \Big|_\pp := \frac{d (f \circ \xx)}{dt}\Big|_{t_0}$, where $\xx$ is a curve satisfying $\xx(t_0) = \pp$ and $\frac{d\xx(t)}{dt}\Big|_{t_0} = \vv$.
    \begin{itemize}
        \item Prove that the directional derivative doesn't depend on the curve chosen.
    \end{itemize}
    \item Prove that the limit expression, $\frac{\pd f}{\pd \vv}\Big|_\pp = \lim_{h \rightarrow 0} \frac{f(\pp + h\vv) - f(\pp)}{h}$, of the directional derivative, follows.
    \item For $f:\R^n \rightarrow \R$, define $\pd_i f := \frac{\pd f}{\pd \see_i}$.
    \begin{itemize}
        \item Aside: for $\ff = (f_1, ..., f_n)^\top:\R \rightarrow \R^n$, define $\boldsymbol{\pd}_i \ff := (\pd_i f_1, ..., \pd_i f_n)^\top$.
    \end{itemize}
    \item Prove the chain rule for partial derivatives (often called ``the multivariable chain rule''), $\frac{d (f \circ \xx)(t)}{dt} =\sum_{i = 1}^n \frac{\pd f(\xx)}{\pd x_i} \frac{dx_i(t)}{dt}$.
    \item Define $\nabla_\xx f|_\pp := (\frac{\pd f}{\pd x_1}\Big|_\pp, ..., \frac{\pd f}{\pd x_n}\Big|_\pp)^\top$, so that the multivariable chain rule can be restated as $\frac{\pd f}{\pd \vv} \Big|_\pp = (\underset{\xx}{\nabla} f)|_\pp \cdot \frac{d\xx(t)}{dt}\Big|_{t_0}= \nabla_\xx f|_\pp \cdot \vv$.
    \item Remark that $(\vv, f) \mapsto \frac{\pd f}{\pd \vv} \Big|_\pp$ is linear in both arguments.
    \item Use the fact that $\frac{\pd f}{\pd \vv}\Big|_\pp = \nabla f|_\pp \cdot \vv$ to prove that $\nabla f$ is the direction of greatest increase in $f$, and (somewhat surprisingly) that $-\nabla f$ is the direction of greatest decrease in $f$.
\end{itemize}

Use the previous results in differential calculus to generalize to functions $\R^n \rightarrow \R^m$.

\begin{itemize}
    \item Use the chain rule for partial derivatives to prove a new chain rule: $\frac{d (\ff \circ \xx)(t)}{dt} = \frac{\pd \ff(\xx)}{\pd \xx} \frac{d\xx(t)}{dt}$, where $\frac{\pd \ff(\xx)}{\pd \xx}$ is the matrix whose $i$th row is $\nabla_\xx f_i$. Define $\JJ \ff|_\xx := \frac{\pd \ff(\xx)}{\pd \xx}$.
    \item Aside on how since the $i$th column of $\frac{\pd \ff(\xx)}{\pd \xx}$ is $\frac{\pd \ff(\xx)}{\pd x_i}$, we have $\frac{d (\ff \circ \xx)(t)}{dt} = \sum_{i = 1}^{n} \frac{\pd \ff(\xx)}{\pd x_i} \frac{d x_i(t)}{dt}$.
    \item Define the directional derivative of $\ff:\R^n \rightarrow \R^m$ similarly to before, so $\frac{\pd \ff}{\pd \vv} \Big|_\pp := \frac{d (\ff \circ \xx)}{dt}\Big|_{t_0}$, where $\xx$ is a curve satisfying $\xx(t_0) = \pp$ and $\frac{d\xx(t)}{dt}\Big|_{t_0} = \vv$.
    \item Prove that $\frac{\pd \ff}{\pd \vv}\Big|_\pp$ exists for all $\vv \in \R^n$ iff $\JJ \ff|_\pp$ exists.
    \item Prove that if $\JJ \ff|_\pp$ exists, then $\frac{\pd \ff}{\pd \vv}\Big|_\pp = \JJ \ff|_\pp \vv$.
\end{itemize}

Then, talk about local linear approximations. 

\begin{itemize}
    \item (Theorem from Hubbards). (p. 106). $f: \R \rightarrow \R$ is differentiable at $p \in \R$ iff $(\Delta f)_p(h) := f(p + h) - f(p)$ is approximately a linear function, in the sense that $f$ is differentiable at $p$ iff there is a linear function $h \mapsto hf'(p):\R \rightarrow \R$ such that $\lim_{h \rightarrow 0} \frac{(\Delta f)_p(h) - hf'(p)}{h} = 0$.
    \item (Defn from Spivak). (p. 16). $\ff$ is \textit{differentiable at $\pp \in \R^n$} iff there exists a linear function $\DD \ff|_\pp:\R^n \rightarrow \R^m$ such that $\lim_{\hh \rightarrow \mathbf{0}} \frac{||\ff(\pp + \hh) - \ff(\pp) - \DD \ff|_\pp(\hh)||}{||\hh||} = 0$. The function $\DD \ff|_\pp$ is called the \textit{total derivative}.
    \begin{itemize}
        \item (Original remark). Note that we haven't yet shown $\DD \ff|_\pp$ is unique.
    \end{itemize}
    
    \item Prove that a function $\ff:\R^n \rightarrow \R^m$ is differentiable at $\pp \in \R^n$ iff all directional derivatives at $\pp$ exist and iff $\vv \mapsto \frac{\pd \ff}{\pd \vv}\Big|_\pp$ is linear for all $\vv \in \R^n$.
    \item (Theorem from Spivak). (p. 30). If $\ff = (f_1, ..., f_n)^\top$ is differentiable at $\pp$, then $\JJ \ff|_\pp$ exists.
    \begin{itemize}
        \item (Remark from Hubbard). If $\DD \ff|_\pp$ exists then it is unique, since it's represented by a matrix.
    \end{itemize}
\end{itemize}


For the discussion on differentiability versus continuous differentiability, use Spivak's clear theorems and the Hubbards' examples.

\begin{itemize}
    \item (Remark from Spivak). (p. 30). If $\ff$'s first partial derivatives all exist at $\pp$ then it is not necessarily the case that $\ff$ is differentiable at $\pp$.
    \begin{itemize}
        \item (Example from Hubbard). (p. 122). $f:\R^2 \rightarrow \R$ defined by $f(x, y) = \frac{x^2 y}{x^2 + y^2}$ when $(x, y) \neq (0, 0)$ and $f(x, y) = (0, 0)$ when $(x, y) = (0, 0)$ is continuous and has partial derivatives but isn't differentiable at $(0, 0)$.
    \end{itemize}
    \item (Original). Even if all directional derivatives of $\ff$ exist at $\pp$, it is not necessarily the case that $\ff$ is differentiable at $\pp$.
    \begin{itemize}
        \item (Original example). Let $A = \{ (x, y) \mid \exists x \spc y = x^2 \} - \{(0, 0)\}$, and consider the characteristic function $\chi_A:\R^2 \rightarrow \R^2$ of $A$. Then the directional derivative $\frac{\pd \chi_A}{\pd \vv}\Big|_{\mathbf{0}}$ of $\chi_A$ at $\mathbf{0}$ exists for all $\vv \in \R^2$ but $\chi_A$ is not differentiable at $\mathbf{0}$.
    \end{itemize}
    \item (Remark from Hubbards). (p. 123). If $\ff$ is differentiable at $\pp$, then $\DD \ff|_\pp$ is not guaranteed to be a good approximation.
    \begin{itemize}
        \item (Example from Hubbard). (p. 123). $f:\R \rightarrow \R$ defined by $f(x) = \frac{1}{2}x + x^2 \sin\Big(\frac{1}{x}\Big)$ when $x \neq 0$ and $f(x) = 0$ when $x = 0$ is continuous and differentiable, but its derivative is not a good approximation of it at the origin, since it does not correctly indicate whether $f$ is increasing or decreasing: ``although the derivative at $0$ is positive, the function is not increasing in any neighborhood of $0$ since on any such interval the derivative is negative.''
    \end{itemize}
    \item (Defn from Spivak). (p. 31). $\ff = (f_1, ..., f_n)^\top$ is \textit{continuously differentiable at $\pp \in \R^n$} iff all partial derivatives $\pd_i f_j$ are continuous at $\pp$ and every partial derivative $\pd_i f_j$ is defined in some open set  containing $\pp$.
    \item (Theorem from Spivak). (p. 31). If $\ff$ is continuously differentiable at $\pp$ then $\ff$ is differentiable at $\pp$.
\end{itemize}   

Now we state rules about $\DD \ff|_\pp$.

\begin{itemize}
    \item (Theorem from Spivak). (p. 20). Rules about $\DD \ff|_\pp$.
    \item (Theorem from Spivak). (p. 32). If $\ff = (f_1, ..., f_m)^\top:\R^n \rightarrow \R^m$ is such that each $f_i$ is continuously differentiable at $\pp$ and $f:\R^m \rightarrow \R$ is differentiable at $\ff(\pp)$, then $\pd_i (f \circ \ff)|_\pp = \sum_{j = 1}^m (\pd_j f)|_{\ff(\pp)} \pd_i (f_j)|_\pp$.
    \begin{itemize}
        \item (Original). In the situation of the above theorem, define the notation $\frac{\pd f(\ff(\xx))}{\pd f_i(\xx)} := \pd_i f|_{\ff(\xx)}$. With this notation, the above theorem becomes $\frac{\pd(f \circ \ff)(\xx)}{\pd x_i}\Big|_\pp = \sum_{j = 1}^m \frac{\pd f(\ff(\xx))}{\pd f_j(\xx)} \Big|_\pp \frac{\pd f_j(\xx)}{\pd x_i}\Big|_\pp$.
    \end{itemize}
\end{itemize}

\section*{Integration}

\begin{theorem}
    (Fubini's theorem). 

    On p. 61 of \textit{Calculus on Manifolds}, Spivak notes that if $C \subseteq A \times B$, then

    \begin{align*}
        \int_C f = \int_{A \times B} \chi_C f = \int_A \Big( \int_B \chi_C(x, y) f(x, y) dy \Big) dx,
    \end{align*}

    and that the ``main difficulty in deriving expressions for $\int_C f$ will be determining $C \cap (\{x\} \times B)$ for $x \in A$''. [I think that by ``determining $C \cap (\{x\} \times B)$ for $x \in A$'', Spivak means ``determining inequalities in $x, y$ that describe the $C \cap (\{x\} \times B)$.]
\end{theorem}

\begin{deriv}
    (Change of variables).
    
    Let $U, V \subseteq \R^n$ be open balls in $\R^n$ and let $\FF:U \subseteq \R^n \rightarrow V \subseteq \R^n$ be a sufficiently differentiable invertible function. Consider the integral

    \begin{align*}
        \int_V f \spc d\text{vol},
    \end{align*}

    where $f:V \subseteq \R^n \rightarrow \R$ is also sufficiently differentiable.

    Now partition $V$ into subsets $\{V_i\}_{i = 1}^n$ and select a point $\yy_i$ from each $V_i$. We have

    \begin{align*}
        \int_V f \spc d\text{vol} \approx \sum_{i = 1}^n f(\yy_i) \text{vol}(V_i).
    \end{align*}

    Since $\FF$ is invertible, there are $\xx_i$ and subsets $\{U_i\}_{i = 1}^n$, with $\xx_i \in U_i$, partitioning $U$ for which $\yy_i = \FF(\xx_i)$ and $V_i = \FF(U_i)$. The thus above becomes

    \begin{align*}
        \sum_i f(\FF(\xx_i)) \text{vol}(\FF(U_i)).
    \end{align*}

    Since the Jacobian is the best linear approximation to $\FF$, when $\xx$ is close to $\xx_i$, we have $\FF(\xx) \approx \FF(\xx_i) + \frac{\pd \FF}{\pd \xx_i}(\xx - \xx_i)$. Since translating a region preserves its volume, the image of each $U_i$ under this approximation has\footnote{$\text{vol}(\FF(\xx_i) + \frac{\pd \FF}{\pd \xx_i}\Big|_{U_i - \xx_i}) = \text{vol}(\FF(\xx_i) + \frac{\pd \FF}{\pd \xx_i}\Big|_{U_i}) = \text{vol}(\frac{\pd \FF}{\pd \xx_i}\Big|_{U_i})$.} the volume $\text{vol}(\frac{\pd \FF}{\pd \xx_i}\Big|_{U_i})$, and so the above is approximately equal to

    \begin{align*}
        \sum_i f(\FF(\xx_i)) \text{vol}\Big(\frac{\pd \FF}{\pd \xx_i}\Big|_{U_i}\Big)
        =
        \sum_i f(\FF(\xx_i)) \det\Big( \frac{\pd \FF}{\pd \xx_i} \Big) \text{vol}(U_i)
        \approx 
        \int_U f(\FF(\xx)) \det\Big( \frac{\pd \FF}{\pd \xx} \Big) d\text{vol}.
    \end{align*}

    Thus

    \begin{align*}
        \int_V f \spc d\text{vol} = \int_U f(\FF(\xx)) \det\Big( \frac{\pd \FF}{\pd \xx} \Big) d \text{vol}
    \end{align*}
\end{deriv}

\section*{Vector calculus}

\begin{defn}
    (Divergence).

    Let $S$ be a closed $3$-dimensional surface containing $\pp \in \R^3$.

    \begin{align*}
        \div(\FF)|_\pp := \lim_{\text{vol}(S) \rightarrow 0} \frac{\int_{\pd S} \FF \cdot \nn dA}{\text{vol}(S)}.
    \end{align*}
\end{defn}

\begin{defn}
    (Curl).

    Let $S$ be a closed $2$-dimensional surface containing $\pp \in \R^3$.

    \begin{align*}
        \text{curl}(\FF)|_\pp := \lim_{\text{vol}(S) \rightarrow 0} \frac{\int_{\pd S}\FF \cdot d\rr}{\text{vol}(S)}.
    \end{align*}
\end{defn}

\begin{theorem}
    (Divergence formula).

    \begin{align*}
        \text{div}(\FF) = \nabla \cdot \FF.
    \end{align*}
\end{theorem}

\begin{proof}
    \url{http://www.supermath.info/CalculusIIIvectorcalculus2011.pdf}

    Proof of $2$-dimensional case.
\end{proof}

\begin{theorem}
    (Curl formula).

    \begin{align*}
        \text{curl}(\FF) = \nabla \times \FF.
    \end{align*}
\end{theorem}

\begin{proof}
    \url{http://www.supermath.info/CalculusIIIvectorcalculus2011.pdf}
\end{proof}

\begin{defn}
    (Irrotaional vector field).
    
    A vector field $\FF$ is called \textit{irrotational} iff $\nabla \times \FF = \mathbf{0}$.
\end{defn}

\begin{theorem}
    (Irrotational vector field).

    A vector field $\FF$ is irrotational iff the following equivalent conditions hold:
    
    \begin{enumerate}
        \item $\int_C \FF \cdot d\rr$ is independent of $C$
        \item If $C$ is a closed curve then $\int_C \FF \cdot d\rr = 0$.
        \item There exists $V$ such that $\FF = -\nabla V$
    \end{enumerate}
\end{theorem}

\begin{remark}
    (Irrotational equals conservative).

    A vector field is called \textit{conservative} iff for all closed curves $C$ we have $\int_C \FF \cdot d\rr = 0$. The above theorem shows that the set of irrotational vector fields is equal to the set of conservative vector fields.
\end{remark}

\begin{defn}
    (Solenoidal vector field).

    A vector field $\FF$ is called \textit{solenoidal} iff $\nabla \cdot \FF = \mathbf{0}$.
\end{defn}

\begin{theorem}
    (Solenoidal vector field).

    A vector field $\FF$ is solenoidal iff the following equivalent conditions hold:

    \begin{enumerate}
        \item $\int_S \FF \cdot \hat{\nn} dA$ is independent of $S$ if $\pd S$ is fixed
        \item If $S$ is a closed surface then $\int_S \FF \cdot \hat{\nn} dA = 0$.
        \item There exists $\AA$ such that $\FF = \nabla \times \AA$
    \end{enumerate}
\end{theorem}

