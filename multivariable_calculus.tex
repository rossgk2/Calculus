\part*{Multivariable calculus}

\subsection*{References}

\begin{itemize}
    \item \textit{The Calculus of Several Variables} by Robert C Rogers of Nagoya University in Japan: \url{https://www.math.nagoya-u.ac.jp/~richard/teaching/s2016/Ref2.pdf}
    \item \textit{Vector Calculus, Linear Algebra, and Differential Forms} by Hubbard and Hubbard
    \item \textit{Calculus on Manifolds} by Michael Spivak
\end{itemize}

\subsection*{Lin alg teaser}

\begin{itemize}
    \item Copy-paste discussion of $\R^n$ over from linear algebra (or find a way to allow this and the linear algebra file to use same dependency)
    \item Define linear function, emphasize that contains the vectors doesn't have to be $\R^n$, the set of ``vectors'' could be a set of functions, example- $\frac{d}{dx}$ and $\int$ are linear functions, but \textit{don't} define vector space
    \item Matrix-vector product and matrix-matrix product
    \item Dot product
    \item Cross product
    \begin{itemize}
        \item For $\vv \in \R^3$, derive the matrix of the linear function $\ww \mapsto \vv \times \ww$ to explain the cross product formula.
        \item Prove that cross product satisfies the ``right hand rule'' (i.e. puts up out of the plane spanned by $\vv$ and $\ww$ if the counterclockwise angle from $\vv$ to $\ww$ is $\leq \frac{\pi}{2}$) by checking it for the standard basis.
        \begin{itemize}
            \item Footnote: in the approach we've used to define the cross product, the right hand rule is imposed by deciding that we should have $\see_1 \times \see_2 = \see_3$, $\see_3 \times \see_1 = \see_2$, and $\see_2 \times \see_3 = \see_1$. When the cross product is defined in the best possible way, the right hand rule is not imposed by the mathematician- it is a natural consequence of other facts. I don't present this best possible way here because it requires knowledge of what are called \textit{determinants}. If you are interested, read [...my lin alg book...] for a presentation of the best possible way.
            \item Point the reader to my linear algebra text for the reason as to why the right hand rule arises. It all starts with noticing that the ordered basis $\{\see_1, \see_2\}$ is rotationally equivalent to $\{-\see_2, \see_1\}$.
        \end{itemize}
    \end{itemize}
\end{itemize}

\newpage

\section*{Differentiation}

\subsection*{Introduction}

There are two approaches to the theory of multivariable differentiation: bottom-up and top-down.

The bottom-up approach has the advantage of requiring no prerequisites other than single-variable calculus. It is often seen in physics contexts, since the bottom-up approach starts with concepts that are both easily digestible and highly relevant to physics (such as the differential and integral calculus of functions $\R \rightarrow \R^n$ and partial derivatives of functions $\R^n \rightarrow \R$). After these digestible concepts are covered, though, study turns to functions $\R^n \rightarrow \R^m$, where a real disadvantage becomes apparent: only at this point, far into the study of multivariable calculus, is a jarring definition of multivariable differentiability (for functions $\R^n \rightarrow \R^m$) given, and the nuanced relationship between differentiability and the existence of partial derivatives is explored.

In the top-down approach, the definition of multivariable differentiability comes early. Because it comes early, it is not jarring- no pattern of derivative definitions has been set yet. The nuanced relationship between differentiability and the existence of partial derivatives is explored immediately, and, even better, the fact that the multivariable derivative (of a function $\R^n \rightarrow \R^m$) is the directional derivative is treated as a centralizing fact instead of a sidenote. 

The top-down approach is clearly superior. Its only disadvantage is that it requires knowledge of linear algebra.

\subsection*{Bottom-up approach}

First, study functions $\R \rightarrow \R^n$.

\begin{itemize}
    \item Define derivatives and integrals of functions $\R^n \rightarrow \R$. 
    \item Study space curves and derive the Frenet frame.
\end{itemize}

Next, study functions $\R^n \rightarrow \R$.
    
\begin{itemize}
    \item Define the directional derivative of $f:\R^n \rightarrow \R$ as $\frac{\pd f}{\pd \vv} \Big|_\pp := \frac{d (f \circ \xx)}{dt}\Big|_{t_0}$, where $\xx$ is a curve satisfying $\xx(t_0) = \pp$ and $\frac{d\xx(t)}{dt}\Big|_{t_0} = \vv$.
    \begin{itemize}
        \item Prove that the directional derivative doesn't depend on the curve chosen.
    \end{itemize}
    \item Prove that the limit expression, $\frac{\pd f}{\pd \vv}\Big|_\pp = \lim_{h \rightarrow 0} \frac{f(\pp + h\vv) - f(\pp)}{h}$, of the directional derivative, follows.
    \item For $f:\R^n \rightarrow \R$, define $\pd_i f := \frac{\pd f}{\pd \see_i}$.
    \begin{itemize}
        \item Aside: for $\ff = (f_1, ..., f_n)^\top:\R \rightarrow \R^n$, define $\boldsymbol{\pd}_i \ff := (\pd_i f_1, ..., \pd_i f_n)^\top$.
    \end{itemize}
    \item Prove the chain rule for partial derivatives (often called ``the multivariable chain rule''), $\frac{d (f \circ \xx)(t)}{dt} =\sum_{i = 1}^n \frac{\pd f(\xx)}{\pd x_i} \frac{dx_i(t)}{dt}$.
    \item Define $\nabla_\xx f|_\pp := (\frac{\pd f}{\pd x_1}\Big|_\pp, ..., \frac{\pd f}{\pd x_n}\Big|_\pp)^\top$, so that application of the multivariable chain rule gives $\frac{\pd f}{\pd \vv} \Big|_\pp = (\underset{\xx}{\nabla} f)|_\pp \cdot \frac{d\xx(t)}{dt}\Big|_{t_0}= \nabla_\xx f|_\pp \cdot \vv$.
    \item Remark that $(\vv, f) \mapsto \frac{\pd f}{\pd \vv} \Big|_\pp$ is linear in both arguments.
    \item Use the fact that $\frac{\pd f}{\pd \vv}\Big|_\pp = \nabla f|_\pp \cdot \vv$ to prove that $\nabla f$ is the direction of greatest increase in $f$, and (somewhat surprisingly) that $-\nabla f$ is the direction of greatest decrease in $f$.
\end{itemize}

Use the previous results in differential calculus to generalize to functions $\R^n \rightarrow \R^m$.

\begin{itemize}
    \item Use the chain rule for partial derivatives to prove a new chain rule: $\frac{d (\ff \circ \xx)(t)}{dt} = \frac{\pd \ff(\xx)}{\pd \xx} \frac{d\xx(t)}{dt}$, where $\frac{\pd \ff(\xx)}{\pd \xx}$ is the matrix whose $i$th row is $\nabla_\xx f_i$.
    \item Aside on how since the $i$th column of $\frac{\pd \ff(\xx)}{\pd \xx}$ is $\frac{\pd \ff(\xx)}{\pd x_i}$, we have $\frac{d (\ff \circ \xx)(t)}{dt} = \sum_{i = 1}^{n} \frac{\pd \ff(\xx)}{\pd x_i} \frac{d x_i(t)}{dt}$.
\end{itemize}

Then, discuss content from the beginning of the top-down approach regarding $\DD \ff$, including the fact that $\DD \ff = \frac{\pd \ff}{\pd \xx}$ when $\ff$ is continuously differentiable.

\subsection*{Top-down approaches}

\subsubsection*{Spivak's \textit{Calculus on Manifolds}}

Let $\ff:\R^n \rightarrow \R^m$.

\begin{itemize}
    \item (Defn). (p. 16). $\ff$ is \textit{differentiable at $\pp \in \R^n$} iff there exists a linear function $\DD \ff|_\pp:\R^n \rightarrow \R^m$ such that $\lim_{\hh \rightarrow \mathbf{0}} \frac{||\ff(\xx + \hh) - \ff(\xx) - \DD \ff|_\pp(\hh)||}{||\hh||} = 0$.
    \item (Theorem). (p. 16). If $\DD \ff|_\pp$ exists then it is unique.
    \item (Theorem). (p. 20). Rules about $\DD \ff|_\pp$.
    \item (Defn). (p. 25). For $f:\R^n \rightarrow \R$, define the \textit{$i$th partial derivative $\pd_i f$  of $f$} to be the derivative of $x \mapsto f(x_1, ..., x_{i - 1}, x, x_{i + 1}, ..., x_n)$.
    \item (Theorem). (p. 30). If $\ff = (f_1, ..., f_n)^\top$ is differentiable at $\pp$, then $\pd_i f_j$ exists at $\pp$ for all $i, j$ and the standard matrix of $\DD \ff|_\pp$ is $(\pd_i f_j|_\pp)$.
    \item (Remark). (p. 30). If $\ff$'s first partial derivatives all exist at $\pp$ then it is not necessarily the case that $\ff$ is differentiable at $\pp$.
    \item (Defn). (p. 31). $\ff = (f_1, ..., f_n)^\top$ is \textit{continuously differentiable at $\pp \in \R^n$} iff all partial derivatives $\pd_i f_j$ are continuous at $\pp$ and every partial derivative $\pd_i f_j$ is defined in some open set containing $\pp$.
    \item (Theorem). (p. 31). If $\ff$ is continuously differentiable at $\pp$ then $\ff$ is differentiable at $\pp$.
    \item (Theorem). (p. 32). If $\ff = (f_1, ..., f_m)^\top:\R^n \rightarrow \R^m$ is such that each $f_i$ is continuously differentiable at $\pp$ and $g:\R^m \rightarrow \R$ is differentiable at $\ff(\pp)$, then $\pd_i (g \circ \ff)|_\pp = \sum_{j = 1}^m (\pd_j g)|_{\ff(\pp)} \pd_i (f_j)|_\pp$.
\end{itemize}

\subsubsection*{Hubbard \& Hubbard}

Let $\ff:\R^n \rightarrow \R^m$.

\begin{itemize}
    \item (Defn). (p. 103). Define the \textit{$i$th partial derivative $\boldsymbol{\pd}_i \ff|_\pp$ of $\ff:\R^n \rightarrow \R^m$} to be $\boldsymbol{\pd}_i \ff|_\pp := \lim_{h \rightarrow 0} \frac{\ff(\pp + h \see_i) - \ff(\pp)}{h}$. When $m = 1$ (so $f := \ff:\R^n \rightarrow \R$) we use an unbolded $\pd_i$.
    \item (Defn). (p. 104). Define the \textit{directional derivative $\frac{\pd \ff}{\pd \vv}\Big|_\pp$ of $\ff:\R^n \rightarrow \R^m$ at $\pp$ along $\vv$} to be $\frac{\pd \ff}{\pd \vv}\Big|_\pp := \lim_{h \rightarrow 0} \frac{\ff(\pp + h \vv) - \ff(\pp)}{h}$.
    \item (Theorem). (p. 106). $f: \R \rightarrow \R$ is differentiable at $p \in \R$ iff $(\Delta f)_p(h) := f(p + h) - f(p)$ is approximately a linear function, in the sense that $f$ is differentiable at $p$ iff there is a linear function $h \mapsto hf'(p):\R \rightarrow \R$ such that $\lim_{h \rightarrow 0} \frac{(\Delta f)_p(h) - hf'(p)}{h} = 0$.
    \item (Defn, theorem). (p. 108, 114). A confusing presentation of Spivak's definition of differentiability from p. 16. and Spivak's theorem about existance of partial derivatives from p. 16.
    \item (Defn). (p. 108). Define $\JJ \ff|_\pp$ to be the standard matrix of $\DD \ff|_\pp$.
    \item (Theorem). (p. 109). If $\ff$ is differentiable at $\pp$ then $\frac{\pd \ff}{\pd \vv}\Big|_\pp = \DD \ff|_\pp(\vv) = \JJ \ff|_\pp \vv$.
    \item (Remark). (p. 115). If $\DD \ff|_\pp$ exists then it is unique, since it's represented by a matrix.
    \item (Theorem). (p. 115). Rules about $\DD \ff|_\pp$.
    \item (Defn). (p. 124). Gives a more restrictive definition of continuous differentiablity than in Spivak that requires continuity on an open set rather than at a point.
    \item (Theorem). (p. 124). If $\ff$ is continuously differentiable on an open set then it is differentiable on that set.
    \item (Example). (p. 122). $f:\R^2 \rightarrow \R$ defined by $f(x, y) = \frac{x^2 y}{x^2 + y^2}$ when $(x, y) \neq (0, 0)$ and $f(x, y) = (0, 0)$ when $(x, y) = (0, 0)$ is continuous and has partial derivatives but isn't differentiable at $(0, 0)$.
    \item (Example). (p. 123). $f:\R \rightarrow \R$ defined by $f(x) = \frac{1}{2}x + x^2 \sin\Big(\frac{1}{x}\Big)$ when $x \neq 0$ and $f(x) = 0$ when $x = 0$ is continuous and differentiable, but its derivative is not a good approximation of it at the origin: ``although the derivative at $0$ is positive, the function is not increasing in any neighborhood of $0$ since on any such interval the derivative is negative.''
\end{itemize}

\subsection*{Proposed approach}

Start with Hubbards' approach for the most part, but use Spivak's clearer definition of differentiability:
\begin{itemize}
    \item (Defn from Hubbards). (p. 104). Define the \textit{directional derivative $\frac{\pd \ff}{\pd \vv}\Big|_\pp$ of $\ff:\R^n \rightarrow \R^m$ at $\pp$ along $\vv$} to be $\frac{\pd \ff}{\pd \vv}\Big|_\pp := \lim_{h \rightarrow 0} \frac{\ff(\pp + h \vv) - \ff(\pp)}{h}$.
    \item (Original). (p. 103). Define the \textit{$i$th partial derivative $\boldsymbol{\pd}_i \ff|_\pp$ of $\ff:\R^n \rightarrow \R^m$} to be $\boldsymbol{\pd}_i \ff|_\pp := \frac{\pd \ff}{\pd \see_i}\Big|_\pp$. Also define the notation $\frac{\boldsymbol{\pd} \ff(x_1, ..., x_n)}{\boldsymbol{\pd} x_i}\Big|_\pp = \frac{\pd \ff(\xx)}{\pd x_i}\Big|_\pp := \boldsymbol{\pd}_i \ff|_\pp$. With both notations, when $m = 1$ (so $f := \ff:\R^n \rightarrow \R$), we use an unbolded $\pd_i$.
    \item (Theorem from Hubbards). (p. 106). $f: \R \rightarrow \R$ is differentiable at $p \in \R$ iff $(\Delta f)_p(h) := f(p + h) - f(p)$ is approximately a linear function, in the sense that $f$ is differentiable at $p$ iff there is a linear function $h \mapsto hf'(p):\R \rightarrow \R$ such that $\lim_{h \rightarrow 0} \frac{(\Delta f)_p(h) - hf'(p)}{h} = 0$.
    \item (Defn from Spivak). (p. 16). $\ff$ is \textit{differentiable at $\pp \in \R^n$} iff there exists a linear function $\DD \ff|_\pp:\R^n \rightarrow \R^m$ such that $\lim_{\hh \rightarrow \mathbf{0}} \frac{||\ff(\pp + \hh) - \ff(\pp) - \DD \ff|_\pp(\hh)||}{||\hh||} = 0$. The function $\DD \ff|_\pp$ is called the \textit{total derivative}.
    \begin{itemize}
        \item (Original remark). Note that we haven't yet shown $\DD \ff|_\pp$ is unique.
    \end{itemize}
\end{itemize}

Follow the Hubbards in presenting the fact that when both exist, the total derivative and directional derivative agree.

\begin{itemize}
    \item (Theorem from Spivak). (p. 30). If $\ff = (f_1, ..., f_n)^\top$ is differentiable at $\pp$, then $\pd_i f_j$ exists at $\pp$ for all $i, j$ and the standard matrix of $\DD \ff|_\pp$ is $(\frac{\pd f_j(\xx)}{\pd x_i})$. This matrix is called the \textit{Jacobian (of $\ff$ at $\pp$)} and is denoted by either $\JJ \ff|_\pp$ or $\frac{\pd \ff}{\pd \xx}\Big|_\pp$.
    \item (Theorem from Hubbard). (p. 109). If $\ff$ is differentiable at $\pp$ then $\frac{\pd \ff}{\pd \vv}\Big|_\pp = \DD \ff|_\pp(\vv) = \frac{\pd \ff}{\pd \xx}\Big|_\pp \vv$.
    \begin{itemize}
        \item (Remark from Hubbard). If $\DD \ff|_\pp$ exists then it is unique, since it's represented by a matrix.
    \end{itemize}
\end{itemize}

For the discussion on differentiability versus continuous differentiability, use Spivak's clear theorems and the Hubbards' examples.

\begin{itemize}
    \item (Remark from Spivak). (p. 30). If $\ff$'s first partial derivatives all exist at $\pp$ then it is not necessarily the case that $\ff$ is differentiable at $\pp$.
    \begin{itemize}
        \item (Example from Hubbard). (p. 122). $f:\R^2 \rightarrow \R$ defined by $f(x, y) = \frac{x^2 y}{x^2 + y^2}$ when $(x, y) \neq (0, 0)$ and $f(x, y) = (0, 0)$ when $(x, y) = (0, 0)$ is continuous and has partial derivatives but isn't differentiable at $(0, 0)$.
    \end{itemize}
    \item (Original). Even if all directional derivatives of $\ff$ exist at $\pp$, it is not necessarily the case that $\ff$ is differentiable at $\pp$.
    \begin{itemize}
        \item (Original example). Let $A = \{ (x, y) \mid \exists x \spc y = x^2 \} - \{(0, 0)\}$, and consider the characteristic function $\chi_A:\R^2 \rightarrow \R^2$ of $A$. Then the directional derivative $\frac{\pd \chi_A}{\pd \vv}\Big|_{\mathbf{0}}$ of $\chi_A$ at $\mathbf{0}$ exists for all $\vv \in \R^2$ but $\chi_A$ is not differentiable at $\mathbf{0}$.
    \end{itemize}
    \item (Remark from Hubbards). (p. 123). If $\ff$ is differentiable at $\pp$, then $\DD \ff|_\pp$ is not guaranteed to be a good approximation.
    \begin{itemize}
        \item (Example from Hubbard). (p. 123). $f:\R \rightarrow \R$ defined by $f(x) = \frac{1}{2}x + x^2 \sin\Big(\frac{1}{x}\Big)$ when $x \neq 0$ and $f(x) = 0$ when $x = 0$ is continuous and differentiable, but its derivative is not a good approximation of it at the origin, since it does not correctly indicate whether $f$ is increasing or decreasing: ``although the derivative at $0$ is positive, the function is not increasing in any neighborhood of $0$ since on any such interval the derivative is negative.''
    \end{itemize}
    \item (Defn from Spivak). (p. 31). $\ff = (f_1, ..., f_n)^\top$ is \textit{continuously differentiable at $\pp \in \R^n$} iff all partial derivatives $\pd_i f_j$ are continuous at $\pp$ and every partial derivative $\pd_i f_j$ is defined in some open set  containing $\pp$.
    \item (Theorem from Spivak). (p. 31). If $\ff$ is continuously differentiable at $\pp$ then $\ff$ is differentiable at $\pp$.
\end{itemize}   

Now we state rules about $\DD \ff|_\pp$.

\begin{itemize}
    \item (Theorem from Spivak). (p. 20). Rules about $\DD \ff|_\pp$.
    \item (Theorem from Spivak). (p. 32). If $\ff = (f_1, ..., f_m)^\top:\R^n \rightarrow \R^m$ is such that each $f_i$ is continuously differentiable at $\pp$ and $f:\R^m \rightarrow \R$ is differentiable at $\ff(\pp)$, then $\pd_i (f \circ \ff)|_\pp = \sum_{j = 1}^m (\pd_j f)|_{\ff(\pp)} \pd_i (f_j)|_\pp$.
    \begin{itemize}
        \item (Original). In the situation of the above theorem, define the notation $\frac{\pd f(\ff(\xx))}{\pd f_i(\xx)} := \pd_i f|_{\ff(\xx)}$. With this notation, the above theorem becomes $\frac{\pd(f \circ \ff)(\xx)}{\pd x_i}\Big|_\pp = \sum_{j = 1}^m \frac{\pd f(\ff(\xx))}{\pd f_j(\xx)} \Big|_\pp \frac{\pd f_j(\xx)}{\pd x_i}\Big|_\pp$.
    \end{itemize}
\end{itemize}

Then prove results about functions $\R^n \rightarrow \R$:

\begin{itemize}
    \item (Original). For $f:\R^n \rightarrow \R$, we define $\nabla f := (\DD f)^\top$.
    \item (Remark). $(\vv, f) \mapsto \frac{\pd f}{\pd \vv} \Big|_\pp$ is linear in both arguments.
    \item (Theorem). Use the fact that $\frac{\pd f}{\pd \vv}\Big|_\pp = \nabla f|_\pp \cdot \vv$ to prove that $\nabla f$ is the direction of greatest increase in $f$, and (somewhat surprisingly) that $-\nabla f$ is the direction of greatest decrease in $f$.
\end{itemize}

\newpage

\section*{Integration}

\begin{theorem}
    (Fubini's theorem). 

    On p. 61 of \textit{Calculus on Manifolds}, Spivak notes that if $C \subseteq A \times B$, then

    \begin{align*}
        \int_C f = \int_{A \times B} \chi_C f = \int_A \Big( \int_B \chi_C(x, y) f(x, y) dy \Big) dx,
    \end{align*}

    and that the ``main difficulty in deriving expressions for $\int_C f$ will be determining $C \cap (\{x\} \times B)$ for $x \in A$''. [I think that by ``determining $C \cap (\{x\} \times B)$ for $x \in A$'', Spivak means ``determining inequalities in $x, y$ that describe the $C \cap (\{x\} \times B)$.]
\end{theorem}

\begin{deriv}
    (Change of variables).
    
    Let $U, V \subseteq \R^n$ be open balls in $\R^n$ and let $\FF:U \subseteq \R^n \rightarrow V \subseteq \R^n$ be a sufficiently differentiable invertible function. Consider the integral

    \begin{align*}
        \int_V f \spc d\text{vol},
    \end{align*}

    where $f:V \subseteq \R^n \rightarrow \R$ is also sufficiently differentiable.

    Now partition $V$ into subsets $\{V_i\}_{i = 1}^n$ and select a point $\yy_i$ from each $V_i$. We have

    \begin{align*}
        \int_V f \spc d\text{vol} \approx \sum_{i = 1}^n f(\yy_i) \text{vol}(V_i).
    \end{align*}

    Since $\FF$ is invertible, there are $\xx_i$ and subsets $\{U_i\}_{i = 1}^n$, with $\xx_i \in U_i$, partitioning $U$ for which $\yy_i = \FF(\xx_i)$ and $V_i = \FF(U_i)$. The thus above becomes

    \begin{align*}
        \sum_i f(\FF(\xx_i)) \text{vol}(\FF(U_i)).
    \end{align*}

    Since the Jacobian is the best linear approximation to $\FF$, when $\xx$ is close to $\xx_i$, we have $\FF(\xx) \approx \FF(\xx_i) + \frac{\pd \FF}{\pd \xx_i}(\xx - \xx_i)$. Since translating a region preserves its volume, the image of each $U_i$ under this approximation has\footnote{$\text{vol}(\FF(\xx_i) + \frac{\pd \FF}{\pd \xx_i}\Big|_{U_i - \xx_i}) = \text{vol}(\FF(\xx_i) + \frac{\pd \FF}{\pd \xx_i}\Big|_{U_i}) = \text{vol}(\frac{\pd \FF}{\pd \xx_i}\Big|_{U_i})$.} the volume $\text{vol}(\frac{\pd \FF}{\pd \xx_i}\Big|_{U_i})$, and so the above is approximately equal to

    \begin{align*}
        \sum_i f(\FF(\xx_i)) \text{vol}\Big(\frac{\pd \FF}{\pd \xx_i}\Big|_{U_i}\Big)
        =
        \sum_i f(\FF(\xx_i)) \det\Big( \frac{\pd \FF}{\pd \xx_i} \Big) \text{vol}(U_i)
        \approx 
        \int_U f(\FF(\xx)) \det\Big( \frac{\pd \FF}{\pd \xx} \Big) d\text{vol}.
    \end{align*}

    Thus

    \begin{align*}
        \int_V f \spc d\text{vol} = \int_U f(\FF(\xx)) \det\Big( \frac{\pd \FF}{\pd \xx} \Big) d \text{vol}
    \end{align*}
\end{deriv}

\section*{Vector calculus}

\begin{defn}
    (Divergence).

    Let $S$ be a closed $3$-dimensional surface containing $\pp \in \R^3$.

    \begin{align*}
        \div(\FF)|_\pp := \lim_{\text{vol}(S) \rightarrow 0} \frac{\int_{\pd S} \FF \cdot \nn dA}{\text{vol}(S)}.
    \end{align*}
\end{defn}

\begin{defn}
    (Curl).

    Let $S$ be a closed $2$-dimensional surface containing $\pp \in \R^3$.

    \begin{align*}
        \text{curl}(\FF)|_\pp := \lim_{\text{vol}(S) \rightarrow 0} \frac{\int_{\pd S}\FF \cdot d\rr}{\text{vol}(S)}.
    \end{align*}
\end{defn}

\begin{theorem}
    (Divergence formula).

    \begin{align*}
        \text{div}(\FF) = \nabla \cdot \FF.
    \end{align*}
\end{theorem}

\begin{proof}
    \url{http://www.supermath.info/CalculusIIIvectorcalculus2011.pdf}

    Proof of $2$-dimensional case.
\end{proof}

\begin{theorem}
    (Curl formula).

    \begin{align*}
        \text{curl}(\FF) = \nabla \times \FF.
    \end{align*}
\end{theorem}

\begin{proof}
    \url{http://www.supermath.info/CalculusIIIvectorcalculus2011.pdf}
\end{proof}

\begin{defn}
    (Irrotaional vector field).
    
    A vector field $\FF$ is called \textit{irrotational} iff $\nabla \times \FF = \mathbf{0}$.
\end{defn}

\begin{theorem}
    (Irrotational vector field).

    A vector field $\FF$ is irrotational iff the following equivalent conditions hold:
    
    \begin{enumerate}
        \item $\int_C \FF \cdot d\rr$ is independent of $C$
        \item If $C$ is a closed curve then $\int_C \FF \cdot d\rr = 0$.
        \item There exists $V$ such that $\FF = -\nabla V$
    \end{enumerate}
\end{theorem}

\begin{remark}
    (Irrotational equals conservative).

    A vector field is called \textit{conservative} iff for all closed curves $C$ we have $\int_C \FF \cdot d\rr = 0$. The above theorem shows that the set of irrotational vector fields is equal to the set of conservative vector fields.
\end{remark}

\begin{defn}
    (Solenoidal vector field).

    A vector field $\FF$ is called \textit{solenoidal} iff $\nabla \cdot \FF = \mathbf{0}$.
\end{defn}

\begin{theorem}
    (Solenoidal vector field).

    A vector field $\FF$ is solenoidal iff the following equivalent conditions hold:

    \begin{enumerate}
        \item $\int_S \FF \cdot \hat{\nn} dA$ is independent of $S$ if $\pd S$ is fixed
        \item If $S$ is a closed surface then $\int_S \FF \cdot \hat{\nn} dA = 0$.
        \item There exists $\AA$ such that $\FF = \nabla \times \AA$
    \end{enumerate}
\end{theorem}

